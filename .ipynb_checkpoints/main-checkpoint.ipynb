{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Stock_Dataset.ipynb\n",
      "importing Jupyter notebook from feature_extract.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\USER\\\\JupyterProjects\\\\AdvLSTM')\n",
    "from Stock_Dataset import StockDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\JupyterProjects\\AdvLSTM\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Att_LSTM.ipynb\n",
      "importing Jupyter notebook from attention.ipynb\n",
      "importing Jupyter notebook from metric.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "from Att_LSTM import attLSTM\n",
    "import numpy as np\n",
    "import time\n",
    "from metric import metric_acc as ACC\n",
    "from metric import metric_mcc as MCC\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def train(attLSTM, lstm_optimizer,Partition, args): ## Data, loss function, argument\n",
    "    trainloader = DataLoader(Partition['train'],\n",
    "                             batch_size = args.batch_size,\n",
    "                             shuffle=False, drop_last=True)\n",
    "\n",
    "    attLSTM.train()\n",
    "\n",
    "    train_loss = 0.0\n",
    "    for i, (x,y) in enumerate(trainloader):\n",
    "        \n",
    "        lstm_optimizer.zero_grad()\n",
    "\n",
    "        linear1 = nn.Linear(11,10)\n",
    "        tanh1 = nn.Tanh()\n",
    "\n",
    "        input_x = tanh1(linear1(x.float())).transpose(0,1).float().to(args.device)\n",
    "        true_y = y.squeeze().float().to(args.device)\n",
    "\n",
    "        attLSTM.hidden = [hidden.to(args.device) for hidden in attLSTM.init_hidden()]\n",
    "        y_pred_lstm, attention_weight, attn_applied = attLSTM(input_x)\n",
    "\n",
    "        # output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "        # output_.requires_grad=True\n",
    "        loss = args.loss_fn(y_pred_lstm, true_y)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        lstm_optimizer.step()## parameter 갱신\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    return attLSTM, train_loss\n",
    "\n",
    "\n",
    "def validation(attLSTM, partition, args):\n",
    "    valloader = DataLoader(partition['val'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    attLSTM.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(valloader):\n",
    "\n",
    "            attLSTM.train()\n",
    "\n",
    "            linear1 = nn.Linear(11, 10)\n",
    "            tanh1 = nn.Tanh()\n",
    "\n",
    "            input_x = tanh1(linear1(x.float())).transpose(0, 1).float().to(args.device)\n",
    "            true_y = y.squeeze().float().to(args.device)\n",
    "\n",
    "            attLSTM.hidden = [attLSTM.to(args.device) for hidden in attLSTM.init_hidden()]\n",
    "\n",
    "            y_pred_lstm, attention_weight, attn_applied = attLSTM(input_x)\n",
    "\n",
    "            loss = args.loss_fn(y_pred_lstm, true_y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        return attLSTM, val_loss\n",
    "\n",
    "\n",
    "def test(attLSTM,partition, args):\n",
    "    testloader = DataLoader(partition['test'],\n",
    "                           batch_size=args.batch_size,\n",
    "                           shuffle=False, drop_last=True)\n",
    "    attLSTM.eval()\n",
    "\n",
    "    ACC_metric = 0.0\n",
    "    MCC_metric = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(testloader):\n",
    "\n",
    "            # feature transform\n",
    "            linear1 = nn.Linear(11, 10)\n",
    "            tanh1 = nn.Tanh()\n",
    "\n",
    "            input_x = tanh1(linear1(x.float())).transpose(0, 1).float().to(args.device)\n",
    "            true_y = y.squeeze().float().to(args.device)\n",
    "\n",
    "            attLSTM.hidden = [hidden.to(args.device) for hidden in attLSTM.init_hidden()]\n",
    "\n",
    "            y_pred_lstm, attention_weight, attn_applied = attLSTM(input_x)\n",
    "\n",
    "            output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\n",
    "\n",
    "            output_.requires_grad = True\n",
    "\n",
    "            ACC_metric += ACC(output_, true_y)\n",
    "            MCC_metric += MCC(output_, true_y)\n",
    "\n",
    "        ACC_metric = ACC_metric / len(testloader)\n",
    "        MCC_metric = MCC_metric / len(testloader)\n",
    "\n",
    "        return ACC_metric, MCC_metric\n",
    "\n",
    "\n",
    "\n",
    "def experiment(partition, args):\n",
    "    attLSTM = args.attLSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                           args.dropout, args.use_bn, args.attention_head, args.attn_size,\n",
    "                           activation=\"ReLU\")\n",
    "\n",
    "    attLSTM.to(args.device)\n",
    "\n",
    "    if args.optim == 'SGD':\n",
    "        lstm_optimizer = optim.SGD(attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'RMSprop':\n",
    "        lstm_optimizer = optim.RMSprop(attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    elif args.optim == 'Adam':\n",
    "        lstm_optimizer = optim.Adam(attLSTM.parameters(), lr=args.lr, weight_decay=args.l2)\n",
    "    else:\n",
    "        raise ValueError('In-valid optimizer choice')\n",
    "\n",
    "    # ===== List for epoch-wise data ====== #\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    # ===================================== #\n",
    "    for epoch in range(args.epoch):\n",
    "        ts = time.time()\n",
    "        attLSTM, train_loss = train(attLSTM, lstm_optimizer, partition, args)\n",
    "\n",
    "        attLSTM, val_loss = validation(attLSTM, partition, args)\n",
    "\n",
    "        te = time.time()\n",
    "\n",
    "        ## 각 에폭마다 모델을 저장하기 위한 코드\n",
    "        torch.save(attLSTM.state_dict(), args.new_file_path + '\\\\' + str(epoch) +'_attLSTM' +'.pt')\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print('Epoch {}, Loss(train/val) {:2.5f}/{:2.5f}. Took {:2.2f} sec'\n",
    "              .format(epoch, train_loss, val_loss, te - ts))\n",
    "\n",
    "    ## val_losses에서 가장 값이 최소인 위치를 저장함\n",
    "    site_val_losses = val_losses.index(min(val_losses)) ## 10 epoch일 경우 0번째~9번째 까지로 나옴\n",
    "    attLSTM = args.attLSTM(args.input_dim, args.hid_dim, args.output_dim, args.num_layers, args.batch_size,\n",
    "                           args.dropout, args.use_bn, args.attention_head, args.attn_size,\n",
    "                           activation=\"ReLU\")\n",
    "    attLSTM.to(args.device)\n",
    "\n",
    "    attLSTM.load_state_dict(torch.load(args.new_file_path + '\\\\' + str(site_val_losses) +'_attLSTM'+ '.pt'))\n",
    "\n",
    "    ACC, MCC = test(attLSTM,partition, args)\n",
    "    print('ACC: {}, MCC: {}'.format(ACC, MCC))\n",
    "\n",
    "    with open(args.new_file_path + '\\\\'+ str(site_val_losses)+'Epoch_test_metric' +'.csv', 'w') as fd:\n",
    "        print('ACC: {}, MCC: {}'.format(ACC, MCC), file=fd)\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    result['train_losses'] = train_losses\n",
    "    result['val_losses'] = val_losses\n",
    "    result['ACC'] = ACC\n",
    "    result['MCC'] = MCC\n",
    "\n",
    "    return vars(args), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '^KS11' : KOSPI\n",
    "# '^KQ11' : 코스닥\n",
    "# '^IXIC' : 나스닥\n",
    "# '^GSPC' : SNP 500 지수\n",
    "# '^DJI' : 다우존수 산업지수\n",
    "# '^HSI' : 홍콩 항생 지수\n",
    "# '^N225' : 니케이지수\n",
    "# '^GDAXI' : 독일 DAX\n",
    "# '^FTSE' : 영국 FTSE\n",
    "# '^FCHI' : 프랑스 CAC\n",
    "# '^IBEX' : 스페인 IBEX\n",
    "# '^TWII' : 대만 기권\n",
    "# '^AEX' : 네덜란드 AEX\n",
    "# '^BSESN' : 인도 센섹스\n",
    "# 'RTSI.ME' : 러시아 RTXI\n",
    "# '^BVSP' : 브라질 보베스파 지수\n",
    "# 'GC=F' : 금 가격\n",
    "# 'CL=F' : 원유 가격 (2000/ 8 / 20일 부터 데이터가 있음)\n",
    "# 'BTC-USD' : 비트코인 암호화폐\n",
    "# 'ETH-USD' : 이더리움 암호화폐\n",
    "\n",
    "# ====== Random Seed Initialization ====== #\n",
    "seed = 666\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# ========= experiment setting ========== #\n",
    "parser = argparse.ArgumentParser()\n",
    "args = parser.parse_args(\"\")\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.save_file_path = \"C:\\\\Users\\\\USER\\\\JupyterProjects\\\\AdvLSTM\\\\results\"\n",
    "\n",
    "## ======== data ============= #\n",
    "# ['CHL', 'DCM' , 'DOW', 'NTT' , 'SYT', 'TOT'] 안되는거\n",
    "# args.stock_list =  ['AAPL', 'AMZN', 'BA', 'BAC', 'BHP', 'BRK-B', 'CMCSA', 'CVX', 'D','DIS','DUK', 'EXC', 'GE', 'GOOGL', 'HD', 'INTC', 'JNJ', 'JPM', 'KO', 'MA', 'MMM', 'MO', 'MRK', 'MSFT', 'NGG','NVS', 'ORCL', 'PEP', 'PFE', 'PG', 'PTR', 'RDS-B', 'RIO', 'SO', 'SPY', 'T', 'TM', 'UNH', 'UPS', 'VALE', 'VZ', 'WFC', 'WMT', 'XOM']\n",
    "args.stock_list =  ['AMZN']\n",
    "#, 'BA', 'BAC']\n",
    "args.data_count = len(args.stock_list)\n",
    "args.train_start = \"2007-01-03\"\n",
    "args.train_end = \"2015-01-01\"\n",
    "args.validation_start = \"2015-01-01\"\n",
    "args.validation_end = \"2016-01-03\"\n",
    "args.test_start = \"2016-01-04\"\n",
    "args.test_end = \"2016-12-31\"\n",
    "\n",
    "# ====== hyperparameter ======= #\n",
    "args.batch_size = 128\n",
    "args.x_frames = 10\n",
    "args.y_frames = 1\n",
    "args.input_dim = 10\n",
    "args.output_dim = 1\n",
    "args.dropout = 0.15\n",
    "args.use_bn = True\n",
    "args.loss_fn = nn.BCELoss()  ## loss function for classification : cross entropy\n",
    "args.optim = 'Adam'\n",
    "args.lr = 0.0001\n",
    "args.l2 = 0.00001 #?\n",
    "args.epoch = 1\n",
    "# ============= model ================== #\n",
    "args.attLSTM = attLSTM\n",
    "\n",
    "# ====== att_lstm hyperparameter ======= #\n",
    "args.hid_dim = 10\n",
    "args.attention_head = 1\n",
    "args.attn_size = 10\n",
    "args.num_layers = 1\n",
    "args.attLSTM_x_frames = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "torch.Size([1, 128, 10])\n",
      "torch.Size([1, 128, 10])\n",
      "torch.Size([128, 20])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26128/2806133136.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0msetting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0meet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mentire_exp_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meet\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26128/1425907859.py\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(partition, args)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mattLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mattLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_26128/1425907859.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(attLSTM, lstm_optimizer, Partition, args)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# output_ = torch.where(output1 >= 0.5, 1.0, 0.0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;31m# output_.requires_grad=True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_lstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py38_64\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py38_64\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py38_64\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2913\u001b[0m         \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2915\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "## 실행 파일\n",
    "with open(args.save_file_path + '\\\\' + 'AdvLSTM_result_t.csv', 'w', encoding='utf-8', newline='') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([\"model\", \"stock\", \"entire_exp_time\",  \"avg_test_ACC\", \"avg_test_MCC\"])\n",
    "\n",
    "    for stock in args.stock_list:\n",
    "        est = time.time()\n",
    "        setattr(args, 'symbol', stock)\n",
    "        args.new_file_path = args.save_file_path + '\\\\' + \"AdvLSTM_\" + args.symbol\n",
    "        os.makedirs(args.new_file_path)\n",
    "\n",
    "        # 0번째에 index 1번째에 stock 1개가 input으로 들어감\n",
    "        trainset = StockDataset(args.symbol, args.x_frames, args.y_frames,\n",
    "                                args.train_start, args.train_end)\n",
    "        valset = StockDataset(args.symbol, args.x_frames, args.y_frames,\n",
    "                              args.validation_start,args.validation_end)\n",
    "        testset = StockDataset(args.symbol, args.x_frames, args.y_frames,\n",
    "                               args.test_start, args.test_end)\n",
    "        partition = {'train': trainset, 'val': valset, 'test': testset}\n",
    "\n",
    "\n",
    "        setting, result = experiment(partition, args)\n",
    "        eet = time.time()\n",
    "        entire_exp_time = eet - est\n",
    "\n",
    "        fig = plt.figure()\n",
    "        plt.plot(result['train_losses'])\n",
    "        plt.plot(result['val_losses'])\n",
    "        plt.legend(['train_losses', 'val_losses'], fontsize=15)\n",
    "        plt.xlabel('epoch', fontsize=15)\n",
    "        plt.ylabel('loss', fontsize=15)\n",
    "        plt.grid()\n",
    "        plt.savefig(args.new_file_path + '\\\\' + str(args.symbol) + '_fig' + '.png')\n",
    "        plt.close(fig)\n",
    "\n",
    "        # csv파일에 기록하기\n",
    "        wr.writerow([\"AdvLSTM\", args.symbol,entire_exp_time, result['ACC'], result['MCC']])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py38_64",
   "language": "python",
   "name": "py38_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
